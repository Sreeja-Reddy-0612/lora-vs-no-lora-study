## Result

LoRA fine-tuning succeeds with a tiny dataset and limited GPU memory.

Unlike full fine-tuning (Phase 2), LoRA adapts only a small number of parameters,
making training stable and feasible on consumer GPUs.

This validates LoRA as a practical alternative to full fine-tuning.
